{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGBgThS8q8k3"
      },
      "source": [
        "Full name: Nguyễn Quốc Huy\n",
        "\n",
        "Student ID: 20127188"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qdrvDrCrnqz"
      },
      "source": [
        "# HW2: Parallel Execution in CUDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKXB0wA7yhq9"
      },
      "source": [
        "**To compile your file, you can use this command:** \\\n",
        "`nvcc tên-file.cu -o tên-file-chạy` \\\n",
        "***You can use Vietnamese to anwser the questions***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH9lSjFfr3Kw"
      },
      "source": [
        "## Question 1A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aZNqZuECjNso"
      },
      "outputs": [],
      "source": [
        "!nvcc HW2_P1.cu -o HW2_P1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVFUj14OYUyy",
        "outputId": "a4a1401f-76cb-4406-beb9-bcb2dfe8ff0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********GPU info**********\n",
            "Name: Tesla T4\n",
            "Compute capability: 7.5\n",
            "Num SMs: 40\n",
            "Max num threads per SM: 1024\n",
            "Max num warps per SM: 32\n",
            "GMEM: 15835398144 bytes\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "Host time: 52.584415 ms\n",
            "\n",
            "Kernel 1\n",
            "Grid size: 8193, block size: 1024\n",
            "Kernel time = 2.603776 ms\n",
            "CORRECT :)\n",
            "\n",
            "Kernel 2\n",
            "Grid size: 8193, block size: 1024\n",
            "Kernel time = 2.684768 ms\n",
            "CORRECT :)\n",
            "\n",
            "Kernel 3\n",
            "Grid size: 8193, block size: 1024\n",
            "Kernel time = 1.280896 ms\n",
            "CORRECT :)\n"
          ]
        }
      ],
      "source": [
        "!./HW2_P1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XebMjR45-0Io"
      },
      "source": [
        "We can rely on the results to see the calculation time of Kernel 3 > Kernel 1 > Kernel 2. This can be understood simply because Kernel 3 uses blockdim/stride and the blocks are consecutive so it will be easy to access. From there, optimize time. Kernel 1 is faster than Kernel 2 because although Kernel 2 is used to reduce divergence, it does more calculations and takes more time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnDDKCef6eb6",
        "outputId": "cb174d70-afa4-458e-e967-5ae294e48563"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==24378== NVPROF is profiling process 24378, command: ./HW2_P1\n",
            "**********GPU info**********\n",
            "Name: Tesla T4\n",
            "Compute capability: 7.5\n",
            "Num SMs: 40\n",
            "Max num threads per SM: 1024\n",
            "Max num warps per SM: 32\n",
            "GMEM: 15835398144 bytes\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "Host time: 43.348160 ms\n",
            "\n",
            "Kernel 1\n",
            "Grid size: 8193, block size: 1024\n",
            "Kernel time = 2.648704 ms\n",
            "CORRECT :)\n",
            "\n",
            "Kernel 2\n",
            "Grid size: 8193, block size: 1024\n",
            "Kernel time = 2.730208 ms\n",
            "CORRECT :)\n",
            "\n",
            "Kernel 3\n",
            "Grid size: 8193, block size: 1024\n",
            "Kernel time = 1.327456 ms\n",
            "CORRECT :)\n",
            "==24378== Profiling application: ./HW2_P1\n",
            "==24378== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   86.64%  42.828ms         3  14.276ms  14.070ms  14.410ms  [CUDA memcpy HtoD]\n",
            "                    5.45%  2.6940ms         1  2.6940ms  2.6940ms  2.6940ms  reduceBlksKernel2(int*, int*, int)\n",
            "                    5.27%  2.6055ms         1  2.6055ms  2.6055ms  2.6055ms  reduceBlksKernel1(int*, int*, int)\n",
            "                    2.61%  1.2924ms         1  1.2924ms  1.2924ms  1.2924ms  reduceBlksKernel3(int*, int*, int)\n",
            "                    0.01%  6.4320us         3  2.1440us  1.6320us  2.4000us  [CUDA memcpy DtoH]\n",
            "                    0.01%  4.1280us         3  1.3760us  1.3760us  1.3760us  [CUDA memset]\n",
            "      API calls:   68.05%  120.00ms         8  15.000ms     868ns  119.99ms  cudaEventCreate\n",
            "                   25.14%  44.339ms         6  7.3899ms  21.680us  15.023ms  cudaMemcpy\n",
            "                    3.74%  6.6010ms         3  2.2003ms  1.2953ms  2.6969ms  cudaDeviceSynchronize\n",
            "                    2.18%  3.8493ms         6  641.55us  177.57us  1.1045ms  cudaFree\n",
            "                    0.48%  844.22us         6  140.70us  72.935us  241.72us  cudaMalloc\n",
            "                    0.15%  267.91us         8  33.488us  5.7240us  82.378us  cudaEventSynchronize\n",
            "                    0.07%  132.17us       101  1.3080us     143ns  52.549us  cuDeviceGetAttribute\n",
            "                    0.05%  93.921us         1  93.921us  93.921us  93.921us  cudaGetDeviceProperties\n",
            "                    0.04%  65.128us         3  21.709us  19.375us  26.351us  cudaLaunchKernel\n",
            "                    0.02%  39.518us         8  4.9390us  2.3690us  10.595us  cudaEventRecord\n",
            "                    0.02%  36.084us         3  12.028us  11.033us  13.676us  cudaMemset\n",
            "                    0.02%  28.328us         1  28.328us  28.328us  28.328us  cuDeviceGetName\n",
            "                    0.01%  22.166us         8  2.7700us     718ns  8.4090us  cudaEventDestroy\n",
            "                    0.00%  7.6020us         4  1.9000us  1.6770us  2.1820us  cudaEventElapsedTime\n",
            "                    0.00%  6.3500us         1  6.3500us  6.3500us  6.3500us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.8400us         3     613ns     319ns  1.1640us  cuDeviceGetCount\n",
            "                    0.00%  1.1700us         2     585ns     312ns     858ns  cuDeviceGet\n",
            "                    0.00%  1.1670us         3     389ns     325ns     458ns  cudaGetLastError\n",
            "                    0.00%     532ns         1     532ns     532ns     532ns  cuModuleGetLoadingMode\n",
            "                    0.00%     424ns         1     424ns     424ns     424ns  cuDeviceTotalMem\n",
            "                    0.00%     379ns         1     379ns     379ns     379ns  cuDeviceGetUuid\n"
          ]
        }
      ],
      "source": [
        "!nvprof ./HW2_P1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irue8vl5b247"
      },
      "source": [
        "**Memory Transfers Dominant:**\n",
        "\n",
        "The majority of GPU time is spent on memory transfers from the host to the device.\n",
        "Reducing the size of data transferred or optimizing memory transfer patterns may help improve overall performance.\n",
        "\n",
        "**Kernel Execution Times:**\n",
        "\n",
        "The kernel execution times for Kernels 1, 2, and 3 are relatively small compared to memory transfers.\n",
        "The efficiency of Kernel 3 is evident as it has the lowest execution time among the three kernels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9wMWgeV--5b"
      },
      "source": [
        "## Question 1B\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMsckPIh_Ije",
        "outputId": "d414164b-2aa6-4494-b191-2b975bdd56f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********GPU info**********\n",
            "Name: Tesla T4\n",
            "Compute capability: 7.5\n",
            "Num SMs: 40\n",
            "Max num threads per SM: 1024\n",
            "Max num warps per SM: 32\n",
            "GMEM: 15835398144 bytes\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "Host time: 45.377537 ms\n",
            "\n",
            "Kernel 1\n",
            "Grid size: 8193, block size: 1024\n",
            "Kernel time = 2.591584 ms\n",
            "CORRECT :)\n",
            "\n",
            "Kernel 2\n",
            "Grid size: 8193, block size: 1024\n",
            "Kernel time = 2.676768 ms\n",
            "CORRECT :)\n",
            "\n",
            "Kernel 3\n",
            "Grid size: 8193, block size: 1024\n",
            "Kernel time = 1.275616 ms\n",
            "CORRECT :)\n"
          ]
        }
      ],
      "source": [
        "!./HW2_P1 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydvO00hC_JMW",
        "outputId": "33a8560b-fbdf-4f53-da19-df7fa1440355"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********GPU info**********\n",
            "Name: Tesla T4\n",
            "Compute capability: 7.5\n",
            "Num SMs: 40\n",
            "Max num threads per SM: 1024\n",
            "Max num warps per SM: 32\n",
            "GMEM: 15835398144 bytes\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "Host time: 43.587967 ms\n",
            "\n",
            "Kernel 1\n",
            "Grid size: 16385, block size: 512\n",
            "Kernel time = 1.882080 ms\n",
            "CORRECT :)\n",
            "\n",
            "Kernel 2\n",
            "Grid size: 16385, block size: 512\n",
            "Kernel time = 1.977216 ms\n",
            "CORRECT :)\n",
            "\n",
            "Kernel 3\n",
            "Grid size: 16385, block size: 512\n",
            "Kernel time = 1.007616 ms\n",
            "CORRECT :)\n"
          ]
        }
      ],
      "source": [
        "!./HW2_P1 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoxamhSa_Jjc",
        "outputId": "51b504f5-3858-4531-95c0-a2aba9f86e63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********GPU info**********\n",
            "Name: Tesla T4\n",
            "Compute capability: 7.5\n",
            "Num SMs: 40\n",
            "Max num threads per SM: 1024\n",
            "Max num warps per SM: 32\n",
            "GMEM: 15835398144 bytes\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "Host time: 44.224545 ms\n",
            "\n",
            "Kernel 1\n",
            "Grid size: 32769, block size: 256\n",
            "Kernel time = 1.533952 ms\n",
            "CORRECT :)\n",
            "\n",
            "Kernel 2\n",
            "Grid size: 32769, block size: 256\n",
            "Kernel time = 1.585152 ms\n",
            "CORRECT :)\n",
            "\n",
            "Kernel 3\n",
            "Grid size: 32769, block size: 256\n",
            "Kernel time = 0.845760 ms\n",
            "CORRECT :)\n"
          ]
        }
      ],
      "source": [
        "!./HW2_P1 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF_Kjjqe_J3F",
        "outputId": "5da1d3eb-16a1-42b9-ee47-c50fe1bedd18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********GPU info**********\n",
            "Name: Tesla T4\n",
            "Compute capability: 7.5\n",
            "Num SMs: 40\n",
            "Max num threads per SM: 1024\n",
            "Max num warps per SM: 32\n",
            "GMEM: 15835398144 bytes\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "Host time: 49.709057 ms\n",
            "\n",
            "Kernel 1\n",
            "Grid size: 65537, block size: 128\n",
            "Kernel time = 1.346752 ms\n",
            "CORRECT :)\n",
            "\n",
            "Kernel 2\n",
            "Grid size: 65537, block size: 128\n",
            "Kernel time = 1.373056 ms\n",
            "CORRECT :)\n",
            "\n",
            "Kernel 3\n",
            "Grid size: 65537, block size: 128\n",
            "Kernel time = 0.746912 ms\n",
            "CORRECT :)\n"
          ]
        }
      ],
      "source": [
        "!./HW2_P1 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9PXsn1C_L4L"
      },
      "source": [
        "Block size | Grid size | Num blocks / SM | Occupancy (%) | Kernel time (ms)\n",
        "--- | --- | --- | --- | ---\n",
        "1024 | 8193 | 1024/1024 = 1 |  | 2.591584\n",
        "512 | 16385 | 1024/512 = 2 |  | 1.882080\n",
        "256 | 32769 | 1024/256 = 4 |  | 1.533952\n",
        "128 | 65537 | 1024/128 = 8 |  | 1.346752"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c811YinAqrd"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh8P0wJlrCkD"
      },
      "source": [
        "## Question 2A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG1mOFWW7zlB"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vbDf8XqIGFW6"
      },
      "outputs": [],
      "source": [
        "!nvcc HW2_P2.cu -o HW2_P2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IfmucpNibmm",
        "outputId": "e1c1e22e-d12b-4a97-9a00-d35cd3e9750a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********GPU info**********\n",
            "Name: Tesla T4\n",
            "Compute capabili_ty: 7.5\n",
            "Num SMs: 40\n",
            "Max num threads per SM: 1024\n",
            "Max num warps per SM: 32\n",
            "GMEM: 15835398144 _bytes\n",
            "****************************\n",
            "\n",
            "Processing time (use host): 2479.260254 ms\n",
            "\n",
            "Basic Matrix Multiplication:\n",
            "Grid size: 32 * 32, block size: 32 * 32\n",
            "Processing time (use device): 8.028256 ms\n",
            "Error between device result and host result: 0.000002\n",
            "\n",
            "Shared memory Matrix Multiplication:\n",
            "Grid size: 32 * 32, block size: 32 * 32\n",
            "Processing time (use device): 5.282016 ms\n",
            "Error between device result and host result: 0.000002"
          ]
        }
      ],
      "source": [
        "!./HW2_P2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lt8SlEGKIOgW",
        "outputId": "379bf253-245d-4094-dcf1-1f265880a1c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==4039== NVPROF is profiling process 4039, command: ./HW2_P2\n",
            "**********GPU info**********\n",
            "Name: Tesla T4\n",
            "Compute capability: 7.5\n",
            "Num SMs: 40\n",
            "Max num threads per SM: 1024\n",
            "Max num warps per SM: 32\n",
            "GMEM: 15835398144 bytes\n",
            "****************************\n",
            "\n",
            "Processing time (use host): 3827.293945 ms\n",
            "\n",
            "Basic Matrix Multiplication:\n",
            "Grid size: 32 * 32, block size: 32 * 32\n",
            "Processing time (use device): 8.042592 ms\n",
            "Error between device result and host result: 0.000002\n",
            "\n",
            "Shared memory Matrix Multiplication:\n",
            "Grid size: 32 * 32, block size: 32 * 32\n",
            "Processing time (use device): 5.379104 ms\n",
            "Error between device result and host result: 0.000002==4039== Profiling application: ./HW2_P2\n",
            "==4039== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   34.72%  3.2686ms         1  3.2686ms  3.2686ms  3.2686ms  matrix_multiplication_kernel1(float*, float*, float*, int, int, int)\n",
            "                   28.05%  2.6408ms         1  2.6408ms  2.6408ms  2.6408ms  matrix_multiplication_kernel2(float*, float*, float*, int, int, int)\n",
            "                   24.89%  2.3430ms         2  1.1715ms  715.50us  1.6275ms  [CUDA memcpy DtoH]\n",
            "                   12.34%  1.1615ms         4  290.37us  235.99us  364.28us  [CUDA memcpy HtoD]\n",
            "      API calls:   92.16%  162.62ms         6  27.104ms  1.0050us  162.60ms  cudaEventCreate\n",
            "                    6.72%  11.861ms         6  1.9768ms  435.77us  6.2175ms  cudaMemcpy\n",
            "                    0.42%  739.61us         6  123.27us  112.94us  148.00us  cudaFree\n",
            "                    0.39%  685.59us         6  114.26us  66.057us  223.02us  cudaMalloc\n",
            "                    0.10%  175.08us       101  1.7330us     194ns  68.506us  cuDeviceGetAttribute\n",
            "                    0.08%  144.66us         1  144.66us  144.66us  144.66us  cudaGetDeviceProperties\n",
            "                    0.04%  70.149us         6  11.691us  3.3530us  24.774us  cudaEventRecord\n",
            "                    0.03%  59.733us         2  29.866us  27.907us  31.826us  cudaLaunchKernel\n",
            "                    0.02%  42.127us         6  7.0210us  5.3500us  10.028us  cudaEventSynchronize\n",
            "                    0.02%  28.217us         1  28.217us  28.217us  28.217us  cuDeviceGetName\n",
            "                    0.01%  12.080us         6  2.0130us     783ns  3.6020us  cudaEventDestroy\n",
            "                    0.00%  7.5080us         1  7.5080us  7.5080us  7.5080us  cuDeviceGetPCIBusId\n",
            "                    0.00%  7.3630us         3  2.4540us  2.1750us  2.8050us  cudaEventElapsedTime\n",
            "                    0.00%  2.2080us         3     736ns     319ns  1.5130us  cuDeviceGetCount\n",
            "                    0.00%  1.2040us         2     602ns     297ns     907ns  cuDeviceGet\n",
            "                    0.00%     756ns         1     756ns     756ns     756ns  cuDeviceTotalMem\n",
            "                    0.00%     373ns         1     373ns     373ns     373ns  cuModuleGetLoadingMode\n",
            "                    0.00%     334ns         1     334ns     334ns     334ns  cuDeviceGetUuid\n"
          ]
        }
      ],
      "source": [
        "!nvprof ./HW2_P2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pKsX-85NNUQ"
      },
      "source": [
        "**Kernel Optimization:**\n",
        "\n",
        "The kernel execution times seem reasonable\n",
        "\n",
        "**Data Transfer Overhead:**\n",
        "\n",
        "The significant time spent on data transfers suggests that optimizing data movement between the host and device could lead to performance improvements\n",
        "\n",
        "**Memory Allocation Overhead:**\n",
        "\n",
        "Memory allocation and deallocation times are relatively low, but depending on the application, optimizing memory management could still provide some benefits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwzjJVcZE2Yc"
      },
      "source": [
        "## Question 2B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xc_-wGJrCkF"
      },
      "source": [
        "**For Basic Matrix Multipication**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRBKekyPrCkF"
      },
      "source": [
        "1. How many floating operations are being performed in your matrix multiply\n",
        "kernel? Explain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6XZdzAlrCkG"
      },
      "source": [
        "The number of floating-point operations per element in the resulting matrix (C) is 2 (one multiplication and one addition). Therefore, the total number of floating-point operations in the kernel is\n",
        "2 × number of elements in C = 2 × m × k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNktNS-NrCkG"
      },
      "source": [
        "2. How many global memory reads are being performed by your kernel? Explain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzfziSaPrCkG"
      },
      "source": [
        "In each thread, there are two reads from global memory for matrices A and B. Therefore, the total number of global memory reads in the kernel is 2 × number of threads = 2 × (m × n + n × k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4VuxoP8rCkG"
      },
      "source": [
        "3. How many global memory writes are being performed by your kernel? Explain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKjnUemrrCkH"
      },
      "source": [
        "In each thread, there is one write to global memory for matrix C. Therefore, the total number of global memory writes in the kernel is number of threads = m × k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kpi9ZANrCkI"
      },
      "source": [
        "**For Tiled Matrix Multipication**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64QDjr8urCkI"
      },
      "source": [
        "1. How many floating operations are being performed in your matrix multiply\n",
        "kernel? Explain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN0E9LNvrCkJ"
      },
      "source": [
        "The number of floating-point operations per element in the resulting matrix (C) is 2 (one multiplication and one addition). Therefore, the total number of floating-point operations in the kernel is m * n * k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iIOI0SVrCkJ"
      },
      "source": [
        "2. How many global memory reads are being performed by your kernel? Explain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JPT6OI9rCkJ"
      },
      "source": [
        "Each thread reads one element from matrices A and B in shared memory (since shared memory is used for optimization). Therefore, the total number of global memory reads in the kernel is 2 * (n/blocksize) * (m/blocksize) * (k/blocksize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOenDpGzrCkK"
      },
      "source": [
        "3. How many global memory writes are being performed by your kernel? Explain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJKeqi0LrCkK"
      },
      "source": [
        "Each thread writes one element to matrix C in global memory. Therefore, the total number of global memory writes in the kernel is number of threads = m × k"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
